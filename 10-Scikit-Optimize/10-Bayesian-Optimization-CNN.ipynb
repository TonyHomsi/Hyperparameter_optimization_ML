{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d4c08f48-fe23-4ddb-ac46-d97f05397514",
    "_uuid": "f2156d1dd26a1243e18512002e10872c5bd7271e"
   },
   "source": [
    "# Bayesian Optimization of CNN with Scikit-Optimize\n",
    "\n",
    "In this notebook, we will use **Bayesian Optimization** to select the best **hyperparameters** for a CNN that recognizes digits in images, using the MNIST dataset and the open source Python package [Scikit-Optimize](https://scikit-optimize.github.io/stable/index.html).\n",
    "\n",
    "The MNIST dataset is availale in [Kaggle](https://www.kaggle.com/c/digit-recognizer/data).\n",
    "\n",
    "## Download dataset\n",
    "\n",
    "- Navigate to the [MNIST website in Kaggle](https://www.kaggle.com/c/digit-recognizer/data)\n",
    "- Download the train.csv file\n",
    "- Unzip and copy the train.csv file to where you see the SAVE_DATASETS-HERE.txt file\n",
    "- Rename to mnist.csv\n",
    "\n",
    "**Remember that you need to be logged in to be able to download the dataset**\n",
    "\n",
    "## Notebook content\n",
    "\n",
    "- Data Preparation\n",
    "- Set up a simple CNN\n",
    "- Set up the hyperparameter search shape\n",
    "- Set up the objective function\n",
    "- Perform Bayesian Optimization\n",
    "- Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "f67b9393-8ea1-4e23-b856-2ce149cfe421",
    "_execution_state": "idle",
    "_uuid": "72334cb006d02a4bcfc2a2fe622524eba824c6f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Data Scientist\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "#from keras import utils\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6d2fb3e6-ab71-4974-b5a2-4af1ebdb99f4",
    "_execution_state": "idle",
    "_uuid": "86061d98eccaa02efe0dab0fa3884e71fcf4c310"
   },
   "source": [
    "#  Data Preparation\n",
    "\n",
    "The dataset contains information about images, each image is a hand-written digit. The aim is to have the computer predict which digit was written by the person, automatically, by \"looking\" at the image. \n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width (28 x 28), making a total of 784 pixels. Each pixel value is an integer between 0 and 255, indicating the darkness in a gray-scale of that pixel.\n",
    "\n",
    "The data is stored in a dataframe where each each pixel is a column (so it is flattened and not in the 28 x 28 format). \n",
    "\n",
    "The data set the has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "5e51d00e-62fd-4141-bf73-50ac4f2da7d0",
    "_execution_state": "idle",
    "_uuid": "84bbd5ab8d7895bd430d5ecfe2f7ddf77baa7b74"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "data = pd.read_csv(\"../train.csv\")\n",
    "\n",
    "# first column is the target, the rest of the columns\n",
    "# are the pixels of the image\n",
    "\n",
    "# each row is 1 image\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37800, 784), (4200, 784))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset into a train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(['label'], axis=1), # the images\n",
    "    data['label'], # the target\n",
    "    test_size = 0.1,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "86570a36-5c20-460a-9dfd-2070548532a7",
    "_execution_state": "idle",
    "_uuid": "1213b979d5ed3e0d13824d17d694c79d2ece92fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of images')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# number of images for each digit\n",
    "\n",
    "g = sns.countplot(x=y_train)\n",
    "plt.xlabel('Digits')\n",
    "plt.ylabel('Number of images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5aea4062-1790-4987-b739-c4bebd79030f",
    "_uuid": "b7b1b1d36243c885e57374c8b60c5a7e10abe922"
   },
   "source": [
    "There are roughly the same amount of images for each of the 10 digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6812040d-80ad-43d2-a571-275f4f20067b",
    "_uuid": "2954681f25f0dcbe986e6914396cdbce61db591f"
   },
   "source": [
    "## Image re-scaling\n",
    "\n",
    "We re-scale data for the CNN, between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "cdc4340b-6e24-4e12-be99-ac806098ff17",
    "_execution_state": "idle",
    "_uuid": "b5d4f8fcf2a967e2c7d57daedf95aa8c5ab7f8cb"
   },
   "outputs": [],
   "source": [
    "# Re-scale the data\n",
    "\n",
    "# 255 is the maximum value a pixel can take\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7413df94-bcb9-4f75-b174-c127d4445766",
    "_uuid": "a66741bf1ac597094f3a3166877008feef27c519"
   },
   "source": [
    "## Reshape\n",
    "\n",
    "The images were stored in a pandas dataframe as 1-D vectors of 784 values. For a CNN with Keras, we need tensors with the following dimensions: width x height x channel. \n",
    "\n",
    "Thus, we reshape all data to 28 x 2 8 x 1, 3-D matrices. \n",
    "\n",
    "The 3rd dimension corresponds to the channel. RGB images have 3 channels. MNIST images are in gray-scale, thus they have only one channel in the 3rd dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "34b6a5f7-8fd2-4387-8ef4-c9dc19584fed",
    "_execution_state": "idle",
    "_uuid": "f0a6ad80dab8e0f2c2e46165ccd9cd82dd162bc3"
   },
   "outputs": [],
   "source": [
    "# Reshape image in 3 dimensions:\n",
    "# height: 28px X width: 28px X channel: 1 \n",
    "\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "X_test = X_test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bdb422e2-bdec-444f-97a5-283a1e54bf2c",
    "_uuid": "39b7a31e843bac6b705461bcce89da216b91799e"
   },
   "source": [
    "## Target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 7, 4, 3, 5, 9, 6, 8, 1], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the target is 1 variable with the 9 different digits\n",
    "# as values\n",
    "\n",
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "4b7f3e78-44dc-4561-b1f0-9429ee024cf4",
    "_execution_state": "idle",
    "_uuid": "cabefd1478d5c1bdfe57fd6a34395340916a854c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Keras, we need to create 10 dummy variables,\n",
    "# one for each digit\n",
    "\n",
    "# Encode labels to one hot vectors (ex : digit 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_test = to_categorical(y_test, num_classes = 10)\n",
    "\n",
    "# the new target\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "adbeacf0-0dc0-4675-b2df-9c9663750f32",
    "_uuid": "60eed15ec5bc0d354385301789ecb8538fc02267"
   },
   "source": [
    "Let's print some example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "5f76131b-4ba0-45f1-a98c-bd4e7d561793",
    "_execution_state": "idle",
    "_uuid": "e0dae8943d3d35f075dba3d7ba31bde1d4bf2ff4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOiklEQVR4nO3de4xc5X3G8efxshhkSGpzcS3bYAIGQtPWhK1JREIduY0AJTGWQoG2KVVMnDZ2BBVqStJIELWKLBpAhURI5iKchotQEhJXRU1clxRBW4eFGrBZYIEaMLZswA2XFOz17q9/7KFZ23veWc85c2Hf70dazcz5zZnz09jPnJl5z5zXESEAk9+UTjcAoD0IO5AJwg5kgrADmSDsQCYOaefGDvXUOEzT2rlJICvv6JfaE7s9Xq1S2G2fI+nvJfVIuiUiVqXuf5im6UwvrrJJAAkbYn1prem38bZ7JH1H0rmSTpN0se3Tmn08AK1V5TP7QknPRsTzEbFH0t2SltTTFoC6VQn7bEkvjbm9tVi2D9vLbffb7h/S7gqbA1BFlbCP9yXAAcfeRsTqiOiLiL5eTa2wOQBVVAn7Vklzx9yeI2lbtXYAtEqVsD8sab7tE2wfKukiSWvraQtA3ZoeeouIvbZXSvqJRofebouIzbV1BqBWlcbZI+I+SffV1AuAFuJwWSAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiATbT2VNFqj55hjSmtPff3E5LqzHko/9vZP7WmmpVpMfe6wZP24b/x7mzqZHNizA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcbZJ4HBK04qrQ189ob0yp+ttu0pDfYXIxpp+rGfPns4Wf/S5suS9Wnf39D0ticj9uxAJgg7kAnCDmSCsAOZIOxAJgg7kAnCDmSCcfZJ4MyzB5pe99mhvcn6ayOHJ+s9DcbR73jt46W1Pzkq/WP606cmy7r7umuT9WXf/1j6ATJTKey2t0h6U9KwpL0R0VdHUwDqV8ee/RMR8WoNjwOghfjMDmSiathD0k9tP2J7+Xh3sL3cdr/t/iHtrrg5AM2q+jb+rIjYZvtYSetsPxURD4y9Q0SslrRakt7nGVFxewCaVGnPHhHbisudku6VtLCOpgDUr+mw255m+8h3r0v6pKRNdTUGoF5V3sbPlHSv7Xcf586I+OdausJBefGak0trZx3zweS6x/7bzmR9+JnnmurpV94prfzRdSuSaz554Y3J+jE96YH4nvkfKK0NDz6fXHcyajrsEfG8pN+usRcALcTQG5AJwg5kgrADmSDsQCYIO5AJfuI6CRz+o5+X1xqsmz5Zc2vNv/Ot9B0urPb4L1zw66W1Od/Mb+iNPTuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5lgnB0d0/PK68l6o+mgGzlqc/o02blhzw5kgrADmSDsQCYIO5AJwg5kgrADmSDsQCYYZ0fXGmkwHXQjew9nXzYWzwaQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5lgnB0d89rHZ7f08Y+8+z9b+vjvNQ337LZvs73T9qYxy2bYXmd7sLic3to2AVQ1kbfxt0s6Z79lV0paHxHzJa0vbgPoYg3DHhEPSNq13+IlktYU19dIOr/etgDUrdkv6GZGxHZJKi6PLbuj7eW2+233D2l3k5sDUFXLv42PiNUR0RcRfb2a2urNASjRbNh32J4lScXlzvpaAtAKzYZ9raRLiuuXSPpxPe0AaJWG4+y275K0SNLRtrdKukrSKkn32F4m6UVJF7SySUxOr3y42vr/8vaR9TSSiYZhj4iLS0qLa+4FQAtxuCyQCcIOZIKwA5kg7EAmCDuQCX7iio75y3P+sdL6X37oD5P1+Xq00uNPNuzZgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBOPsbdBzyknJ+t4Z09rUyYEOeeOdZH1489OVHn/w9jNKa8vevzq57vbh9GnMTrnu7WS92oTPkw97diAThB3IBGEHMkHYgUwQdiAThB3IBGEHMsE4+wT979IzS2vvv+zF5LpfPe6eZL1v6nCyPqXBa/JIhRHlp4fS275m2/5zeu5r49rTkvUfLbqutDainuS6V7706WR95LGBZB37Ys8OZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmGGcvHHL83GR9xarysfKlR+ysu522OaU3PdZ96/HrkvUpX16frDcaS0/Z8N/zkvX5vb9M1mNoT9Pbnowa7tlt32Z7p+1NY5Zdbftl2xuLv/Na2yaAqibyNv52SeMdRnV9RCwo/u6rty0AdWsY9oh4QNKuNvQCoIWqfEG30vbjxdv86WV3sr3cdr/t/iGlzykGoHWaDftNkk6UtEDSdknXlt0xIlZHRF9E9PVqapObA1BVU2GPiB0RMRwRI5JulrSw3rYA1K2psNueNebmUkmbyu4LoDs0HGe3fZekRZKOtr1V0lWSFtleICkkbZH0xda12B7PLZuTrFcZS7/hf05N1v/r9eOS9SmOZH0kXFr7zNEbk+t28zECT34ifV75U29ckayf/Gc/r7Od97yGYY+Ii8dZfGsLegHQQhwuC2SCsAOZIOxAJgg7kAnCDmSCn7gW9sxr/lDeTXvSQ2P3L5qXrA+/+lrT227kW5delKwv/cYNLdt2qz316e8k6ytPX1Rae/mPZybXHR58vomOuht7diAThB3IBGEHMkHYgUwQdiAThB3IBGEHMsE4e2Fw8S3J+kjidbFHe5Pr7p2f/vmsGtR3H50+w89Xr19TWlt8+CPpbVd8vX/ond5k/fP/9IXSWu/r6W3/zYV3JutLp6VPjXjz3IdKa0M/S09V/dGrVibrR93yH8l6N2LPDmSCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJhyR/i12nd7nGXGmF7dtewfj7Z+ckKyv+1D5lM2tNqXBa/KIRtrUyYGWLrowWa/yu/Cemccm61suPSlZf+xLN5bWGj1n976V3vY3B8ab6/RX5lyaPkV3q85hsCHW643YNe65xdmzA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcbZCyO/e3qyftf3vl1aO3LKoXW3s48q4+zb9qbPh3/7L85M1jd8fkGyHo9sTtY7yWf8RmntnO+W/9Zdkv781wYrbfsLL6b/n+/46BuVHr9MpXF223Nt3297wPZm25cVy2fYXmd7sLicXnfjAOozkbfxeyVdEREflPQRSStsnybpSknrI2K+pPXFbQBdqmHYI2J7RDxaXH9T0oCk2ZKWSHr3fEhrJJ3foh4B1OCgvqCzPU/S6ZI2SJoZEdul0RcESeMeTGx7ue1+2/1Dan4+NQDVTDjsto+Q9ANJl0fEhL9diIjVEdEXEX29Sp84EUDrTCjstns1GvQ7IuKHxeIdtmcV9VmS0j/zAdBRDYfebFujn8l3RcTlY5b/naTXImKV7SslzYiIr6Qeq5uH3hp55pa+0trv/eZAct1vz/lZpW03Gnr721d/q7T24F98JLnuIf/a6FTTk1PPyScm6wNfSQ8uPXXuTZW2/5nZv1Np/TKpobeJnDf+LEmfk/SE7Y3Fsq9JWiXpHtvLJL0o6YIaegXQIg3DHhEPShr3lULSe3M3DWSIw2WBTBB2IBOEHcgEYQcyQdiBTPAT1xq4N/0T1zjj1JZuv+epF0prw794vaXbnqwa/Ztu+Xr5cReStGfeO8n6/EsePeieJoJTSQMg7EAuCDuQCcIOZIKwA5kg7EAmCDuQCcbZgUmEcXYAhB3IBWEHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMtEw7Lbn2r7f9oDtzbYvK5Zfbftl2xuLv/Na3y6AZk1kfva9kq6IiEdtHynpEdvritr1EfGt1rUHoC4TmZ99u6TtxfU3bQ9Imt3qxgDU66A+s9ueJ+l0SRuKRSttP277NtvTS9ZZbrvfdv+QdlfrFkDTJhx220dI+oGkyyPiDUk3STpR0gKN7vmvHW+9iFgdEX0R0derqdU7BtCUCYXddq9Gg35HRPxQkiJiR0QMR8SIpJslLWxdmwCqmsi38ZZ0q6SBiLhuzPJZY+62VNKm+tsDUJeJfBt/lqTPSXrC9sZi2dckXWx7gaSQtEXSF1vQH4CaTOTb+AcljXce6vvqbwdAq3AEHZAJwg5kgrADmSDsQCYIO5AJwg5kgrADmSDsQCYIO5AJwg5kgrADmSDsQCYIO5AJwg5kwhHRvo3Zr0h6YcyioyW92rYGDk639tatfUn01qw6ezs+Io4Zr9DWsB+wcbs/Ivo61kBCt/bWrX1J9NasdvXG23ggE4QdyESnw766w9tP6dbeurUvid6a1ZbeOvqZHUD7dHrPDqBNCDuQiY6E3fY5tp+2/aztKzvRQxnbW2w/UUxD3d/hXm6zvdP2pjHLZtheZ3uwuBx3jr0O9dYV03gnphnv6HPX6enP2/6Z3XaPpGck/b6krZIelnRxRDzZ1kZK2N4iqS8iOn4Ahu2zJb0l6bsR8aFi2TWSdkXEquKFcnpE/FWX9Ha1pLc6PY13MVvRrLHTjEs6X9KfqoPPXaKvP1AbnrdO7NkXSno2Ip6PiD2S7pa0pAN9dL2IeEDSrv0WL5G0pri+RqP/WdqupLeuEBHbI+LR4vqbkt6dZryjz12ir7boRNhnS3ppzO2t6q753kPST20/Ynt5p5sZx8yI2C6N/ueRdGyH+9lfw2m822m/aca75rlrZvrzqjoR9vGmkuqm8b+zIuLDks6VtKJ4u4qJmdA03u0yzjTjXaHZ6c+r6kTYt0qaO+b2HEnbOtDHuCJiW3G5U9K96r6pqHe8O4Nucbmzw/38v26axnu8acbVBc9dJ6c/70TYH5Y03/YJtg+VdJGktR3o4wC2pxVfnMj2NEmfVPdNRb1W0iXF9Usk/biDveyjW6bxLptmXB1+7jo+/XlEtP1P0nka/Ub+OUl/3YkeSvr6gKTHir/Nne5N0l0afVs3pNF3RMskHSVpvaTB4nJGF/X2D5KekPS4RoM1q0O9fUyjHw0fl7Sx+Duv089doq+2PG8cLgtkgiPogEwQdiAThB3IBGEHMkHYgUwQdiAThB3IxP8Bq15lA9lEjlsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some image examples \n",
    "\n",
    "g = plt.imshow(X_train[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some image examples \n",
    "\n",
    "g = plt.imshow(X_train[10][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d5265777-aeb3-449d-b171-d88cad74c0a4",
    "_uuid": "5fa18b37a9acd9e098bac1d12264b0dd4310fdd3"
   },
   "source": [
    "# Define the CNN\n",
    "\n",
    "We will create a CNN, with 2 Convolutional layers followed by Pooling, and varying number of fully-connected Dense layers.\n",
    "\n",
    "In fact, the number of fully-connected Dense layers and the number of Neurons in each one of the Dense layers, are some of the hyperparameters we want to optimize.\n",
    "\n",
    "We could also optimize the number of Convolutional layers. But we will keep that for later, and here we keep it a bit simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create the CNN\n",
    "\n",
    "def create_cnn(\n",
    "    learning_rate,\n",
    "    num_dense_layers,\n",
    "    num_dense_nodes,\n",
    "    activation,\n",
    "):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start construction of a Keras Sequential model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # First convolutional layer.\n",
    "    # There are many hyper-parameters in this layer\n",
    "    # For this demo, we will optimize the activation function only.\n",
    "    model.add(Conv2D(kernel_size=5, strides=1, filters=16, padding='same',\n",
    "                     activation=activation, name='layer_conv1'))\n",
    "    model.add(MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "    # Second convolutional layer.\n",
    "    # Again, we will only optimize the activation function.\n",
    "    model.add(Conv2D(kernel_size=5, strides=1, filters=36, padding='same',\n",
    "                     activation=activation, name='layer_conv2'))\n",
    "    model.add(MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "    # Flatten the 4-rank output of the convolutional layers\n",
    "    # to 2-rank that can be input to a fully-connected Dense layer.\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add fully-connected Dense layers.\n",
    "    # The number of layers is a hyper-parameter we want to optimize.\n",
    "    # We add the different number of layers in the following loop:\n",
    "    \n",
    "    for i in range(num_dense_layers):\n",
    "        \n",
    "        # Add the dense fully-connected layer to the model.\n",
    "        # This has two hyper-parameters we want to optimize:\n",
    "        # The number of nodes (neurons) and the activation function.\n",
    "        model.add(Dense(num_dense_nodes,\n",
    "                        activation=activation,\n",
    "                        ))\n",
    "\n",
    "    # Last fully-connected dense layer with softmax-activation\n",
    "    # for use in classification.\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Use the Adam method for training the network.\n",
    "    # We want to find the best learning-rate for the Adam method.\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # In Keras we need to compile the model so it can be trained.\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Hyperparameter Space\n",
    "\n",
    "Scikit-optimize provides an utility function to create the range of values to examine for each hyperparameters. More details in [skopt.Space](https://scikit-optimize.github.io/stable/modules/generated/skopt.Space.html)\n",
    "\n",
    "\n",
    "We want to find the following hyper-parameters:\n",
    "\n",
    "- The learning rate of the optimizer.\n",
    "- The number of fully-connected Dense layers.\n",
    "- The number of nodes (neurons) for each of the dense layers.\n",
    "- Whether to use 'sigmoid' or 'relu' activation in all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_learning_rate = Real(\n",
    "    low=1e-6, high=1e-2, prior='log-uniform', name='learning_rate',\n",
    ")\n",
    "\n",
    "dim_num_dense_layers = Integer(low=1, high=5, name='num_dense_layers')\n",
    "\n",
    "dim_num_dense_nodes = Integer(low=5, high=512, name='num_dense_nodes')\n",
    "\n",
    "\n",
    "dim_activation = Categorical(\n",
    "    categories=['relu', 'sigmoid'], name='activation',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hyperparameter space grid\n",
    "\n",
    "param_grid = [dim_learning_rate,\n",
    "              dim_num_dense_layers,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_activation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will save the model with this name\n",
    "path_best_model = 'cnn_model.h5'\n",
    "\n",
    "# starting point for the optimization\n",
    "best_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(param_grid)\n",
    "def objective(\n",
    "    learning_rate,\n",
    "    num_dense_layers,\n",
    "    num_dense_nodes,\n",
    "    activation,\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('num_dense_layers:', num_dense_layers)\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('activation:', activation)\n",
    "    print()\n",
    "    \n",
    "    # Create the neural network with the hyper-parameters.\n",
    "    # We call the function we created previously.\n",
    "    model = create_cnn(learning_rate=learning_rate,\n",
    "                       num_dense_layers=num_dense_layers,\n",
    "                       num_dense_nodes=num_dense_nodes,\n",
    "                       activation=activation)\n",
    "\n",
    "   \n",
    "    # Set a learning rate annealer\n",
    "    # this reduces the learning rate if learning does not improve\n",
    "    # for a certain number of epochs\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                                patience=2, \n",
    "                                                verbose=1, \n",
    "                                                factor=0.5, \n",
    "                                                min_lr=0.00001)\n",
    "   \n",
    "    # train the model\n",
    "    # we use 3 epochs to be able to run the notebook in a \"reasonable\"\n",
    "    # time. If we increase the epochs, we will have better performance\n",
    "    # this could be another parameter to optimize in fact.\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        epochs=3,\n",
    "                        batch_size=128,\n",
    "                        validation_split=0.1,\n",
    "                        callbacks=learning_rate_reduction)\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_accuracy\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if accuracy > best_accuracy:\n",
    "        # Save the new model to harddisk.\n",
    "        # Training CNNs is costly, so we want to avoid having to re-train\n",
    "        # the network with the best found parameters. We save it instead\n",
    "        # as we search for the best hyperparam space.\n",
    "        model.save(path_best_model)\n",
    "        \n",
    "        # Update the classification accuracy.\n",
    "        best_accuracy = accuracy\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "\n",
    "    \n",
    "    # Remember that Scikit-optimize always minimizes the objective\n",
    "    # function, so we need to negate the accuracy (because we want\n",
    "    # the maximum accuracy)\n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 1.0e-05\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 16\n",
      "activation: relu\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Before we run the hyper-parameter optimization, \n",
    "# let's first check that the everything is working\n",
    "# by passing some default hyper-parameters.\n",
    "\n",
    "default_parameters = [1e-5, 1, 16, 'relu']\n",
    "\n",
    "objective(x=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a mediocre accuracy, but all our code is working. So let's get started with the Optimization now!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b453af8d-9736-43e3-b486-7a1cd7dd8909",
    "_execution_state": "idle",
    "_uuid": "cf36b3d029f95b553be02d612e097a9769ee8252",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gp_minimize performs by default GP Optimization \n",
    "# using a Marten Kernel\n",
    "\n",
    "gp_ = gp_minimize(\n",
    "    objective, # the objective function to minimize\n",
    "    param_grid, # the hyperparameter space\n",
    "    x0=default_parameters, # the initial parameters to test\n",
    "    acq_func='EI', # the acquisition function\n",
    "    n_calls=30, # the number of subsequent evaluations of f(x)\n",
    "    random_state=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function value at the minimum.\n",
    "# note that it is the negative of the accuracy\n",
    "\n",
    "\"Best score=%.4f\" % gp_.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_.space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"Best parameters:\n",
    "=========================\n",
    "- learning rate=%.6f\n",
    "- num_dense_layers=%d\n",
    "- num_nodes=%d\n",
    "- activation = %s\"\"\" % (gp_.x[0], \n",
    "                gp_.x[1],\n",
    "                gp_.x[2],\n",
    "                gp_.x[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(gp_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partially dependency plots\n",
    "\n",
    "[plot_objective](https://scikit-optimize.github.io/stable/modules/generated/skopt.plots.plot_objective.html#skopt.plots.plot_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_names = ['learning_rate', 'num_dense_nodes', 'num_dense_layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_objective(result=gp_, plot_dims=dim_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation order\n",
    "\n",
    "[plot_evaluations](https://scikit-optimize.github.io/stable/modules/generated/skopt.plots.plot_evaluations.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluations(result=gp_, plot_dims=dim_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e758621d-b27b-40ff-a93f-bebd2e0e5243",
    "_uuid": "0a1834f2a9f2db15dcaba4a84004b9627d714469"
   },
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "\n",
    "model = load_model(path_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions in test set\n",
    "\n",
    "result = model.evaluate(x=X_test,\n",
    "                        y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluation metrics\n",
    "\n",
    "for name, value in zip(model.metrics_names, result):\n",
    "    print(name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5688faa0-b33b-4e92-b125-7fa0b37e7df3",
    "_uuid": "3306d29b732341663e50866140dc569360701a81"
   },
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values from the validation dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions classes to one hot vectors \n",
    "y_pred_classes = np.argmax(y_pred, axis = 1)\n",
    "\n",
    "# Convert validation observations to one hot vectors\n",
    "y_true = np.argmax(y_test, axis = 1)\n",
    "\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes) \n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "11361e73-8250-4bf5-a353-b0f8ea83e659",
    "_execution_state": "idle",
    "_uuid": "16e161179bf1b51ba66c39b2cead883f1db3a9c7"
   },
   "outputs": [],
   "source": [
    "# let's make it more colourful\n",
    "classes = 10\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(classes)\n",
    "plt.xticks(tick_marks, range(classes), rotation=45)\n",
    "plt.yticks(tick_marks, range(classes))\n",
    "\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, cm[i, j],\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > 100 else \"black\",\n",
    "            )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1b8a5cdc-9122-4e31-b9fa-0f6b57d33fc8",
    "_uuid": "ecb928433299b163ecc1f6c4e66d4ddcf38fe898"
   },
   "source": [
    "Here we can see that our CNN performs very well on all digits.\n",
    "\n",
    "#  References\n",
    "\n",
    "This notebook was based on these resources:\n",
    "\n",
    "- [TensorFlow Tutorial #19](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb)\n",
    "- [Introduction to CNN Keras - 0.997 (top 6%)](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6)\n",
    "- [Keras](https://keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('logistec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288.467px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "87ccbcd820c85ab21551c003710ca3342a305a01c89eb4274d7e9483918c7dd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
