{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization with Scikit-Optimize\n",
    "\n",
    "In this notebook, we will perform **Bayesian Optimization** with Gaussian Processes in Parallel, utilizing various CPUs, to speed up the search.\n",
    "\n",
    "This is useful to reduce search times. \n",
    "\n",
    "https://scikit-optimize.github.io/stable/auto_examples/parallel-optimization.html#example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from skopt import Optimizer # for the optimization\n",
    "from joblib import Parallel, delayed # for the parallelization\n",
    "\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2       3        4        5       6        7       8   \\\n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "        9   ...     20     21      22      23      24      25      26      27  \\\n",
       "0  0.07871  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.05667  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.05999  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.09744  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.05883  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       28       29  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)\n",
    "X = pd.DataFrame(breast_cancer_X)\n",
    "y = pd.Series(breast_cancer_y).map({0:1, 1:0})\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.627417\n",
       "1    0.372583\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the target:\n",
    "# percentage of benign (0) and malign tumors (1)\n",
    "\n",
    "y.value_counts() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 30), (171, 30))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset into a train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Hyperparameter Space\n",
    "\n",
    "Scikit-optimize provides an utility function to create the range of values to examine for each hyperparameters. More details in [skopt.Space](https://scikit-optimize.github.io/stable/modules/generated/skopt.Space.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the hyperparameter space\n",
    "\n",
    "param_grid = [\n",
    "    Integer(10, 120, name=\"n_estimators\"),\n",
    "    Integer(1, 5, name=\"max_depth\"),\n",
    "    Real(0.0001, 0.1, prior='log-uniform', name='learning_rate'),\n",
    "    Real(0.001, 0.999, prior='log-uniform', name=\"min_samples_split\"),\n",
    "    Categorical(['deviance', 'exponential'], name=\"loss\"),\n",
    "]\n",
    "\n",
    "# Scikit-optimize parameter grid is a list\n",
    "type(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the gradient boosting classifier\n",
    "\n",
    "gbm = GradientBoostingClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the objective function\n",
    "\n",
    "This is the hyperparameter response space, the function we want to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We design a function to maximize the accuracy, of a GBM,\n",
    "# with cross-validation\n",
    "\n",
    "# the decorator allows our objective function to receive the parameters as\n",
    "# keyword arguments. This is a requirement for scikit-optimize.\n",
    "\n",
    "@use_named_args(param_grid)\n",
    "def objective(**params):\n",
    "    \n",
    "    # model with new parameters\n",
    "    gbm.set_params(**params)\n",
    "\n",
    "    # optimization function (hyperparam response function)\n",
    "    value = np.mean(\n",
    "        cross_val_score(\n",
    "            gbm, \n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=3,\n",
    "            n_jobs=-4,\n",
    "            scoring='accuracy')\n",
    "    )\n",
    "\n",
    "    # negate because we need to minimize\n",
    "    return -value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Optimizer\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    dimensions = param_grid, # the hyperparameter space\n",
    "    base_estimator = \"GP\", # the surrogate\n",
    "    n_initial_points=10, # the number of points to evaluate f(x) to start of\n",
    "    acq_func='EI', # the acquisition function\n",
    "    random_state=0, \n",
    "    n_jobs=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\poc\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\ProgramData\\Anaconda3\\envs\\poc\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    }
   ],
   "source": [
    "# we will use 4 CPUs (n_points)\n",
    "# if we loop 10 times using 4 end points, we perform 40 searches in total\n",
    "\n",
    "for i in range(10):\n",
    "    x = optimizer.ask(n_points=4)  # x is a list of n_points points\n",
    "    y = Parallel(n_jobs=4)(delayed(objective)(v) for v in x)  # evaluate points in parallel\n",
    "    optimizer.tell(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[68, 4, 0.007381238832487747, 0.08704800719052391, 'exponential'],\n",
       " [118, 2, 0.00010113718979245275, 0.05725990689319986, 'deviance'],\n",
       " [17, 3, 0.00022221129238269847, 0.17371729808265857, 'exponential'],\n",
       " [42, 4, 0.000960176974739521, 0.6793527084375192, 'exponential'],\n",
       " [38, 5, 0.053126979002083165, 0.06277193933628669, 'deviance'],\n",
       " [28, 4, 0.005445788189169609, 0.002258808366805843, 'deviance'],\n",
       " [56, 1, 0.0006780193306440557, 0.9274466173670369, 'exponential'],\n",
       " [42, 4, 0.08952334707464486, 0.20644568894340298, 'deviance'],\n",
       " [68, 1, 0.0010637763908757617, 0.0037524397848558923, 'deviance'],\n",
       " [48, 4, 0.0016815858162959685, 0.27886812907463643, 'deviance'],\n",
       " [63, 3, 0.0165738266384725, 0.02731983287242903, 'exponential'],\n",
       " [84, 5, 0.004221241394969906, 0.025089109838094856, 'exponential'],\n",
       " [104, 5, 0.015028217636269001, 0.999, 'exponential'],\n",
       " [68, 5, 0.0476932687782069, 0.999, 'exponential'],\n",
       " [86, 4, 0.011586118609782342, 0.999, 'exponential'],\n",
       " [10, 2, 0.07181065980752435, 0.0011551226574021355, 'exponential'],\n",
       " [10, 1, 0.02977143613321738, 0.001, 'exponential'],\n",
       " [120, 5, 0.05376415174711999, 0.999, 'exponential'],\n",
       " [120, 5, 0.1, 0.999, 'exponential'],\n",
       " [120, 1, 0.027547192389723727, 0.999, 'exponential'],\n",
       " [120, 1, 0.009664982058638109, 0.001, 'exponential'],\n",
       " [94, 1, 0.1, 0.001, 'exponential'],\n",
       " [120, 5, 0.1, 0.001, 'deviance'],\n",
       " [111, 1, 0.1, 0.06721241195718895, 'deviance'],\n",
       " [120, 5, 0.1, 0.999, 'exponential'],\n",
       " [108, 1, 0.08093402861973303, 0.999, 'deviance'],\n",
       " [117, 1, 0.06565565051538683, 0.001, 'deviance'],\n",
       " [110, 1, 0.1, 0.008060853485982315, 'deviance'],\n",
       " [120, 1, 0.1, 0.001, 'deviance'],\n",
       " [105, 1, 0.1, 0.001, 'deviance'],\n",
       " [105, 1, 0.1, 0.001, 'deviance'],\n",
       " [10, 1, 0.1, 0.999, 'exponential'],\n",
       " [73, 5, 0.1, 0.001, 'exponential'],\n",
       " [120, 5, 0.029068284119987214, 0.001, 'exponential'],\n",
       " [120, 1, 0.1, 0.999, 'deviance'],\n",
       " [120, 1, 0.1, 0.031107426489723776, 'deviance'],\n",
       " [44, 1, 0.05317039169014555, 0.001, 'exponential'],\n",
       " [119, 1, 0.1, 0.001, 'exponential'],\n",
       " [82, 1, 0.05354086052250857, 0.999, 'exponential'],\n",
       " [37, 5, 0.04750155749609544, 0.999, 'exponential']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the evaluated hyperparamters\n",
    "\n",
    "optimizer.Xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.9171413381939697,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.9296536796536796,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.9321979190400244,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.9397357028935976,\n",
       " -0.9145781119465329,\n",
       " -0.9296346927925875,\n",
       " -0.9472355130249867,\n",
       " -0.9195336826915774,\n",
       " -0.9246411483253588,\n",
       " -0.886914255335308,\n",
       " -0.9447102604997341,\n",
       " -0.9572795625427205,\n",
       " -0.9497607655502392,\n",
       " -0.9195336826915774,\n",
       " -0.9497227918280551,\n",
       " -0.9322358927622085,\n",
       " -0.9572795625427205,\n",
       " -0.9572795625427205,\n",
       " -0.9497607655502392,\n",
       " -0.9497607655502392,\n",
       " -0.9572795625427205,\n",
       " -0.9572795625427205,\n",
       " -0.9522670312143996,\n",
       " -0.9522670312143996,\n",
       " -0.9170274170274171,\n",
       " -0.9271853877117037,\n",
       " -0.9271664008506114,\n",
       " -0.9572795625427205,\n",
       " -0.9572795625427205,\n",
       " -0.9447292473608263,\n",
       " -0.9572795625427205,\n",
       " -0.9497607655502392,\n",
       " -0.9296346927925875]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the accuracy\n",
    "\n",
    "optimizer.yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.057260</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.173717</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.679353</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053127</td>\n",
       "      <td>0.062772</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.929654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_estimators  max_depth  min_samples_split  learning_rate         loss  \\\n",
       "0            68          4           0.007381       0.087048  exponential   \n",
       "1           118          2           0.000101       0.057260     deviance   \n",
       "2            17          3           0.000222       0.173717  exponential   \n",
       "3            42          4           0.000960       0.679353  exponential   \n",
       "4            38          5           0.053127       0.062772     deviance   \n",
       "\n",
       "   accuracy  \n",
       "0 -0.917141  \n",
       "1 -0.625636  \n",
       "2 -0.625636  \n",
       "3 -0.625636  \n",
       "4 -0.929654  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all together in one dataframe, so we can investigate further\n",
    "dim_names = ['n_estimators', 'max_depth', 'min_samples_split', 'learning_rate', 'loss']\n",
    "\n",
    "tmp = pd.concat([\n",
    "    pd.DataFrame(optimizer.Xi),\n",
    "    pd.Series(optimizer.yi),\n",
    "], axis=1)\n",
    "\n",
    "tmp.columns = dim_names + ['accuracy']\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate convergence of the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp['accuracy'].sort_values(ascending=False).reset_index(drop=True).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trade-off with parallelization, is that we will not optimize the search after each evaluation of f(x), instead after, in this case 4, evaluations of f(x). Thus, we may need to perform more evaluations to find the optima. But, because we do it in parallel, overall, we reduce wall time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.031107</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.067212</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.008061</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.952267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.952267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027547</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>0.053541</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>0.065656</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>0.080934</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.949723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>0.047693</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.947236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0.053170</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.944729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053764</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.944710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.027320</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.939736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.932236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.089523</td>\n",
       "      <td>0.206446</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.932198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053127</td>\n",
       "      <td>0.062772</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.929654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>0.047502</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.929635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>104</td>\n",
       "      <td>5</td>\n",
       "      <td>0.015028</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.929635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>73</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.927185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.029068</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.927166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.071811</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.924641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>0.011586</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.919534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009665</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.919534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>84</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.025089</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.914578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029771</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.886914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.278868</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.679353</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.173717</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.057260</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.927447</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  max_depth  min_samples_split  learning_rate         loss  \\\n",
       "37           119          1           0.100000       0.001000  exponential   \n",
       "35           120          1           0.100000       0.031107     deviance   \n",
       "18           120          5           0.100000       0.999000  exponential   \n",
       "34           120          1           0.100000       0.999000     deviance   \n",
       "23           111          1           0.100000       0.067212     deviance   \n",
       "28           120          1           0.100000       0.001000     deviance   \n",
       "24           120          5           0.100000       0.999000  exponential   \n",
       "27           110          1           0.100000       0.008061     deviance   \n",
       "30           105          1           0.100000       0.001000     deviance   \n",
       "29           105          1           0.100000       0.001000     deviance   \n",
       "19           120          1           0.027547       0.999000  exponential   \n",
       "38            82          1           0.053541       0.999000  exponential   \n",
       "26           117          1           0.065656       0.001000     deviance   \n",
       "25           108          1           0.080934       0.999000     deviance   \n",
       "21            94          1           0.100000       0.001000  exponential   \n",
       "13            68          5           0.047693       0.999000  exponential   \n",
       "36            44          1           0.053170       0.001000  exponential   \n",
       "17           120          5           0.053764       0.999000  exponential   \n",
       "10            63          3           0.016574       0.027320  exponential   \n",
       "22           120          5           0.100000       0.001000     deviance   \n",
       "7             42          4           0.089523       0.206446     deviance   \n",
       "4             38          5           0.053127       0.062772     deviance   \n",
       "39            37          5           0.047502       0.999000  exponential   \n",
       "12           104          5           0.015028       0.999000  exponential   \n",
       "32            73          5           0.100000       0.001000  exponential   \n",
       "33           120          5           0.029068       0.001000  exponential   \n",
       "15            10          2           0.071811       0.001155  exponential   \n",
       "14            86          4           0.011586       0.999000  exponential   \n",
       "20           120          1           0.009665       0.001000  exponential   \n",
       "0             68          4           0.007381       0.087048  exponential   \n",
       "31            10          1           0.100000       0.999000  exponential   \n",
       "11            84          5           0.004221       0.025089  exponential   \n",
       "16            10          1           0.029771       0.001000  exponential   \n",
       "8             68          1           0.001064       0.003752     deviance   \n",
       "9             48          4           0.001682       0.278868     deviance   \n",
       "5             28          4           0.005446       0.002259     deviance   \n",
       "3             42          4           0.000960       0.679353  exponential   \n",
       "2             17          3           0.000222       0.173717  exponential   \n",
       "1            118          2           0.000101       0.057260     deviance   \n",
       "6             56          1           0.000678       0.927447  exponential   \n",
       "\n",
       "    accuracy  \n",
       "37 -0.957280  \n",
       "35 -0.957280  \n",
       "18 -0.957280  \n",
       "34 -0.957280  \n",
       "23 -0.957280  \n",
       "28 -0.957280  \n",
       "24 -0.957280  \n",
       "27 -0.957280  \n",
       "30 -0.952267  \n",
       "29 -0.952267  \n",
       "19 -0.949761  \n",
       "38 -0.949761  \n",
       "26 -0.949761  \n",
       "25 -0.949761  \n",
       "21 -0.949723  \n",
       "13 -0.947236  \n",
       "36 -0.944729  \n",
       "17 -0.944710  \n",
       "10 -0.939736  \n",
       "22 -0.932236  \n",
       "7  -0.932198  \n",
       "4  -0.929654  \n",
       "39 -0.929635  \n",
       "12 -0.929635  \n",
       "32 -0.927185  \n",
       "33 -0.927166  \n",
       "15 -0.924641  \n",
       "14 -0.919534  \n",
       "20 -0.919534  \n",
       "0  -0.917141  \n",
       "31 -0.917027  \n",
       "11 -0.914578  \n",
       "16 -0.886914  \n",
       "8  -0.625636  \n",
       "9  -0.625636  \n",
       "5  -0.625636  \n",
       "3  -0.625636  \n",
       "2  -0.625636  \n",
       "1  -0.625636  \n",
       "6  -0.625636  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.sort_values(by='accuracy', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79e45289c8e930fbd824f1a40d2becba2b50a0a7d6605f8ebcdb03fc8bca05f9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('poc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
